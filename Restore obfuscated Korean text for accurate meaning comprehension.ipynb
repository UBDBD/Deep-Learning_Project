{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UBDBD/Deep-Learning_Project/blob/main/Restore%20obfuscated%20Korean%20text%20for%20accurate%20meaning%20comprehension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuiLtm02gOE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee62511-e9c1-4a4d-9090-0e4ac92e288c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ],
      "source": [
        "# 라이브러리\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "from g2pk import G2p\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "fgzjNoRZHQlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLl4k1MYynWg"
      },
      "source": [
        "# G2P"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##발음 처리"
      ],
      "metadata": {
        "id": "a3GC-0xqjazy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izF6fqz8ePOj"
      },
      "outputs": [],
      "source": [
        "# g2p 한글만 변환\n",
        "class CustomG2p(G2p):\n",
        "    def g2p_only_korean(self, text):\n",
        "        return \"\".join([self(char) if re.fullmatch(r'[가-힣]', char) else char for char in text])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# g2p 처리 함수\n",
        "def g2p_process(text):\n",
        "  g2p = CustomG2p()\n",
        "\n",
        "  words = text.split(' ')\n",
        "  g2p_word = [g2p.g2p_only_korean(word) for word in words]\n",
        "  g2p_text = ' '.join(g2p_word)\n",
        "  return g2p_text"
      ],
      "metadata": {
        "id": "FTGFcYrj23Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 데이터 전처리\n"
      ],
      "metadata": {
        "id": "pn3jd5rPjevC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa7T5TrJoVkj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f2c2b38c-147a-4935-cec8-c2a68d9a2afb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# 학습 데이터 전처리\\ndata = pd.read_csv('/content/drive/MyDrive/프로젝트/Deep-Learning/난독화된 한글 리뷰 복원_data/train.csv')\\n\\ndata_dict = {'input': data['input'].tolist(), 'output: data['output'].tolist()}\\n\\nfor i in tqdm(range(len(data['input']))):\\n    input_text = data['input'][i]\\n    g2p_text = g2p_process(input_text)\\n\\n    data_dict['input'][i] = g2p_text\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "'''\n",
        "# 학습 데이터 전처리\n",
        "data = pd.read_csv('./Data/train.csv')\n",
        "\n",
        "data_dict = {'input': data['input'].tolist(), 'output: data['output'].tolist()}\n",
        "\n",
        "for i in tqdm(range(len(data['input']))):\n",
        "    input_text = data['input'][i]\n",
        "    g2p_text = g2p_process(input_text)\n",
        "\n",
        "    data_dict['input'][i] = g2p_text\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# 전처리 저장\n",
        "df = pd.DataFrame(data_dict)\n",
        "df.to_csv('./Data/g2p_data.csv', encoding='utf-8-sig')\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2Dd29dnq-tWt",
        "outputId": "238ec43b-960b-4c36-ce1f-8c35e15e12c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# 전처리 저장\\ndf = pd.DataFrame(data_dict)\\ndf.to_csv('/content/drive/MyDrive/프로젝트/Deep-Learning/난독화된 한글 리뷰 복원_data/g2p_data.csv', encoding='utf-8-sig')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM\n"
      ],
      "metadata": {
        "id": "kAVXLm5-L_kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리"
      ],
      "metadata": {
        "id": "aqfP6zHNjif_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "g2p_data = pd.read_csv('./Data/g2p_data.csv')\n",
        "train_data, test_data = train_test_split(g2p_data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "CZd5z3JFAbw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 음절 분리 함수\n",
        "def split_syllables(text):\n",
        "    return list(text)"
      ],
      "metadata": {
        "id": "cxw1sRggOFbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 변환\n",
        "train_data['input_syllables'] = train_data['input'].apply(split_syllables)\n",
        "train_data['output_syllables'] = train_data['output'].apply(split_syllables)"
      ],
      "metadata": {
        "id": "Xp1lFW3ZODJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 사전\n",
        "char2index = defaultdict(lambda: len(char2index) + 1)\n",
        "index2char = {}\n",
        "\n",
        "if ' ' not in char2index:\n",
        "    char2index[' '] = 1\n",
        "    index2char[1] = ' '\n",
        "\n",
        "for text in pd.concat([train_data['input_syllables'], train_data['output_syllables']]):\n",
        "    for char in text:\n",
        "        if char not in char2index:\n",
        "            index = len(char2index) + 1\n",
        "            char2index[char] = index\n",
        "            index2char[index] = char"
      ],
      "metadata": {
        "id": "bEgKM_hPOB1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 정의\n",
        "class SyllableDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.inputs = data['input_syllables'].tolist()\n",
        "        self.targets = data['output_syllables'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, list):\n",
        "            return [self.__getitem__(i) for i in idx]\n",
        "\n",
        "        input_seq = self.inputs[idx]\n",
        "        target_seq = self.targets[idx]\n",
        "\n",
        "        input_tensor = torch.tensor([char2index.get(ch, char2index[' ']) for ch in input_seq], dtype=torch.long)\n",
        "        target_tensor = torch.tensor([char2index.get(ch, char2index[' ']) for ch in target_seq], dtype=torch.long)\n",
        "\n",
        "        return input_tensor, target_tensor\n"
      ],
      "metadata": {
        "id": "M7tYSJgfN_5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 패딩 함수\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
        "    return inputs_padded.to(device), targets_padded.to(device)"
      ],
      "metadata": {
        "id": "MsvaJzNDN9zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 변환\n",
        "batch_size = 64\n",
        "\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "train_dataset = SyllableDataset(train_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "ZTCB3vx-N53K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##모델 학습"
      ],
      "metadata": {
        "id": "nbV-r9hwjmSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "class SyllableLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(SyllableLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        output = self.fc(lstm_out)\n",
        "        return self.softmax(output)"
      ],
      "metadata": {
        "id": "QOU4zbSSN8L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 설정\n",
        "vocab_size = len(char2index) + 1\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "num_epochs = 10\n",
        "\n",
        "lstm_model = SyllableLSTM(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "y4Uzz6NIN2Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "'''\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for input_batch, target_batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = lstm_model(input_batch)\n",
        "\n",
        "        # 길이 맞춰서 자르기\n",
        "        min_len = min(output.size(1), target_batch.size(1))\n",
        "        output = output[:, :min_len, :].contiguous()\n",
        "        target_batch = target_batch[:, :min_len].contiguous()\n",
        "\n",
        "        loss = criterion(output.reshape(-1, vocab_size), target_batch.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.2f}')\n",
        "    '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "8xykFX-NNymi",
        "outputId": "8665add2-f0a2-41db-a0e8-3cc89cb4c873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntorch.cuda.empty_cache()\\n\\nfor epoch in range(num_epochs):\\n    total_loss = 0\\n    for input_batch, target_batch in tqdm(train_loader):\\n        optimizer.zero_grad()\\n        output = lstm_model(input_batch)\\n\\n        # 길이 맞춰서 자르기\\n        min_len = min(output.size(1), target_batch.size(1))\\n        output = output[:, :min_len, :].contiguous()\\n        target_batch = target_batch[:, :min_len].contiguous()\\n\\n        loss = criterion(output.reshape(-1, vocab_size), target_batch.reshape(-1))\\n        loss.backward()\\n        optimizer.step()\\n        total_loss += loss.item()\\n\\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.2f}')\\n    \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "'''\n",
        "torch.save(lstm_model.state_dict(), './LSTM_model')\n",
        "with open('./char2index.pkl', 'wb') as f:\n",
        "    pickle.dump(dict(char2index), f)\n",
        "'''"
      ],
      "metadata": {
        "id": "LJeAGoMuDwdT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8396e41d-3489-41db-c300-4595d5f6acf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntorch.save(lstm_model.state_dict(), '/content/drive/MyDrive/프로젝트/Deep-Learning/LSTM_model')\\nwith open('/content/drive/MyDrive/프로젝트/Deep-Learning/char2index.pkl', 'wb') as f:\\n    pickle.dump(dict(char2index), f)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 불러오기\n",
        "with open('./char2index.pkl', 'rb') as f:\n",
        "    char2index_loaded = pickle.load(f)\n",
        "\n",
        "char2index = defaultdict(lambda: len(char2index_loaded) + 1)\n",
        "char2index.update(char2index_loaded)\n",
        "\n",
        "lstm_model = SyllableLSTM(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
        "lstm_model.load_state_dict(torch.load('./LSTM_model'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXvBChvGD9LB",
        "outputId": "f6322bf0-f4b2-4abd-c3e0-21b68e67015f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 평가"
      ],
      "metadata": {
        "id": "WGcAjApVjyDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 복원 함수\n",
        "def restored_lstm(model, input_text):\n",
        "    model.eval()\n",
        "\n",
        "    input_encoded = [char2index[char] if char in char2index else char2index[' '] for char in input_text]\n",
        "    input_tensor = torch.tensor(input_encoded, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        predicted_indices = output.argmax(dim=2).squeeze(0).tolist()\n",
        "\n",
        "    valid_indices = [idx if idx in index2char else 1 for idx in predicted_indices]\n",
        "    restored_text = \"\".join([index2char[idx] for idx in valid_indices])\n",
        "\n",
        "    if len(restored_text) < len(input_text):\n",
        "        restored_text += ' ' * (len(input_text) - len(restored_text))\n",
        "    elif len(restored_text) > len(input_text):\n",
        "        restored_text = restored_text[:len(input_text)]\n",
        "\n",
        "    return restored_text"
      ],
      "metadata": {
        "id": "AbAcbp2FNwOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 함수\n",
        "def evaluate_model(preds, targets):\n",
        "    all_num_same, all_pred_len, all_target_len = 0, 0, 0\n",
        "\n",
        "    for pred, target in zip(preds, targets):\n",
        "        length = min(len(pred), len(target))\n",
        "        num_same = sum([1 for i in range(length) if pred[i] == target[i]])\n",
        "\n",
        "        all_num_same += num_same\n",
        "        all_pred_len += len(pred)\n",
        "        all_target_len += len(target)\n",
        "\n",
        "    precision = all_num_same / all_pred_len if all_pred_len > 0 else 0.0\n",
        "    recall = all_num_same / all_target_len if all_target_len > 0 else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "wc5Xa5r7_AFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "lstm_preds = []\n",
        "lstm_targets = []\n",
        "\n",
        "for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
        "    input_text = row['input']\n",
        "    target_text = row['output']\n",
        "\n",
        "    lstm_out = restored_lstm(lstm_model, input_text)\n",
        "\n",
        "    lstm_preds.append(lstm_out)\n",
        "    lstm_targets.append(target_text)\n",
        "\n",
        "lstm_scores = evaluate_model(lstm_preds, lstm_targets)\n",
        "\n",
        "print('')\n",
        "print(f'Model: {lstm_scores}')\n"
      ],
      "metadata": {
        "id": "Skas9TmRBxSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f1b150-1b4f-4185-f39d-e022d7635684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2253/2253 [00:10<00:00, 220.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LSTM: {'precision': 0.9485883591431552, 'recall': 0.9482631365919859, 'f1': 0.9484257199872361}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트\n",
        "input_text = '녀뮨넒뭅 만죡숭러윤 효템뤼에오. 푸싸눼 옰면 콕 츄쩐학꼬 싶은 콧쉰웨오. 췌꾜윕뉘댜! ㅎㅎ 당음웨 또 옭 컷 갗았요.'\n",
        "\n",
        "lstm_start_time = time.time()\n",
        "\n",
        "g2p_text = g2p_process(input_text)\n",
        "restored_text = restored_lstm(lstm_model, g2p_text)\n",
        "\n",
        "lstm_end_time = time.time()\n",
        "lstm_eval_time = lstm_end_time - lstm_start_time\n",
        "\n",
        "print(f'input: {input_text}')\n",
        "print(f'g2p: {g2p_text}')\n",
        "print(f'output: {restored_text}')\n",
        "print(f'time: {lstm_eval_time}')"
      ],
      "metadata": {
        "id": "oBdtD8ALurdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2906412-9ec2-450d-e54f-1e206798cb7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: 녀뮨넒뭅 만죡숭러윤 효템뤼에오. 푸싸눼 옰면 콕 츄쩐학꼬 싶은 콧쉰웨오. 췌꾜윕뉘댜! ㅎㅎ 당음웨 또 옭 컷 갗았요.\n",
            "g2p: 녀뮨넘뭅 만죡숭러윤 효템뤼에오. 푸싸눼 올면 콕 츄쩐학꼬 십은 콛쉰웨오. 췌꾜윕뉘댜! ㅎㅎ 당음웨 또 옥 컫 갇앋요.\n",
            "output: 너무너무 만족스러운 호텔이에요. 부산에 오면 꼭 추천하고 싶은 곳이에요. 최고입니다! ㅎㅎ 다음에 또 올 것 같아요.\n",
            "time: 1.8878514766693115\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1huDps4H1R_ggBVCJr_23KPF6seSyYAta",
      "authorship_tag": "ABX9TyO2w2SVYb2dSnnWoS/8pea3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}